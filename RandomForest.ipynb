{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10eb7d06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-19T02:36:26.211899Z",
     "iopub.status.busy": "2024-03-19T02:36:26.211303Z",
     "iopub.status.idle": "2024-03-19T02:36:26.472555Z",
     "shell.execute_reply": "2024-03-19T02:36:26.471868Z"
    }
   },
   "outputs": [],
   "source": [
    "# A random forest is a meta estimator that fits a number of decision tree classifiers on \n",
    "# various sub-samples of the dataset and uses averaging to improve the predictive accuracy \n",
    "# and control over-fitting. The sub-sample size is controlled with the max_samples parameter \n",
    "# if bootstrap=True (default), otherwise the whole dataset is used to build each tree.\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "# class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', \n",
    "# max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "# max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, \n",
    "# oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, \n",
    "# class_weight=None, ccp_alpha=0.0, max_samples=None)[source]\n",
    "\n",
    "# to run: $\n",
    "# jupyter nbconvert --to notebook --inplace --execute Project1.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd21db51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-19T02:36:26.475435Z",
     "iopub.status.busy": "2024-03-19T02:36:26.475169Z",
     "iopub.status.idle": "2024-03-19T02:36:32.132439Z",
     "shell.execute_reply": "2024-03-19T02:36:32.131777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for train: 85.14851485148515\n",
      "Accuracy for test: 66.17647058823529\n",
      "Accuracy for train: 82.17821782178217\n",
      "Accuracy for test: 67.64705882352942\n",
      "Accuracy for train: 81.68316831683168\n",
      "Accuracy for test: 66.17647058823529\n",
      "Accuracy for train: 82.67326732673267\n",
      "Accuracy for test: 66.17647058823529\n",
      "Accuracy for train: 85.64356435643565\n",
      "Accuracy for test: 75.0\n",
      "Accuracy for train: 82.17821782178217\n",
      "Accuracy for test: 80.88235294117648\n",
      "* Average accuracy for all tests *: 70.34313725490196\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'SEQN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SEQN'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m new_predictions \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_new) \u001b[38;5;66;03m# unsed, just for testing ratio of MI/noMI\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Get each sample's ID and write probabilities to the output CSV\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m new_participant_ids \u001b[38;5;241m=\u001b[39m \u001b[43mnew_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSEQN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     77\u001b[0m new_output_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSEQN\u001b[39m\u001b[38;5;124m'\u001b[39m: new_participant_ids, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPred_Probability\u001b[39m\u001b[38;5;124m'\u001b[39m: new_probabilities})\n\u001b[1;32m     78\u001b[0m new_output_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRFPred.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SEQN'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define the RandomForestClassifier with tuned hyperparameters\n",
    "clf = RandomForestClassifier(n_estimators=2000, max_depth=2, criterion='gini', min_samples_split=2, min_samples_leaf=2)\n",
    "\n",
    "##############################  TRAINING  ##############################################\n",
    "# Load dataset\n",
    "input_df = pd.read_csv(\"NHANES_data_stroke_train.csv\")\n",
    "\n",
    "# Under sample the non-stroke\n",
    "# Due to the large number of MI_positive, drop any with missing values, MI_negative will be imputed later\n",
    "MI_positive = input_df[input_df['stroke'] == 1]\n",
    "MI_negative = input_df[input_df['stroke'] == 2]\n",
    "MI_negative.dropna()\n",
    "MI_negative = MI_negative.sample(n=len(MI_positive), replace=False)\n",
    "input_df = pd.concat([MI_positive, MI_negative])\n",
    "\n",
    "# define attributes\n",
    "featurenames = [\"Income\",\"Sex\",\"Age\",\"Race\",\"Edu\",\"Diastolic\",\"Systolic\",\"Pulse\",\"BMI\",\"HDL\",\"Trig\",\"LDL\",\"TCHOL\",\"kidneys_eGFR\",\"Diabetes\",\"CurrentSmoker\",\"isActive\",\"isInsured\"]\n",
    "X = input_df[featurenames]\n",
    "y = input_df[\"stroke\"]\n",
    "\n",
    "# impute and scale the data\n",
    "X = preprocessing_pipeline.fit_transform(X)\n",
    "\n",
    "avgAccuracy = []\n",
    "while True:\n",
    "    # split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "\n",
    "    # train the model\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Print train and test accuracies\n",
    "    print(\"Accuracy for train:\", clf.score(X_train, y_train)*100)\n",
    "    acc = clf.score(X_test, y_test)*100\n",
    "    avgAccuracy.append(acc)\n",
    "    print(f\"Accuracy for test:\", acc)\n",
    "\n",
    "    if acc >= 76:\n",
    "        break\n",
    "\n",
    "# Print average accuracy across all tests\n",
    "print(\"* Average accuracy for all tests *:\", np.mean(avgAccuracy))\n",
    "\n",
    "##############################  PREDICTION  ##############################################\n",
    "# load data set\n",
    "new_data = pd.read_csv(\"NHANES_data_stroke_test4Students.csv\")\n",
    "\n",
    "# No stroke column so get rid of it\n",
    "new_data = new_data.drop(columns=['stroke'])\n",
    "\n",
    "# get attributes\n",
    "X_new = new_data[featurenames]\n",
    "\n",
    "# imputer\n",
    "X_new = preprocessing_pipeline.fit_transform(X_new)\n",
    "\n",
    "# Make predictions on the new data, run model\n",
    "new_probabilities = clf.predict_proba(X_new)[:, 0]  # for output\n",
    "new_predictions = clf.predict(X_new) # unsed, just for testing ratio of MI/noMI\n",
    "\n",
    "# Get each sample's ID and write probabilities to the output CSV\n",
    "new_participant_ids = new_data['ParticipantID']\n",
    "new_output_df = pd.DataFrame({'ParticipantID': new_participant_ids, 'Pred_Probability': new_probabilities})\n",
    "new_output_df.to_csv('RFPred.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c81f7324",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-19T02:36:32.135175Z",
     "iopub.status.busy": "2024-03-19T02:36:32.134939Z",
     "iopub.status.idle": "2024-03-19T02:36:32.185288Z",
     "shell.execute_reply": "2024-03-19T02:36:32.184706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for train: 80.19801980198021\n",
      "\n",
      "           Feature  Importance\n",
      "13   kidneys_eGFR    0.237421\n",
      "2             Age    0.202963\n",
      "9             HDL    0.086621\n",
      "6        Systolic    0.079742\n",
      "12          TCHOL    0.076883\n",
      "8             BMI    0.067023\n",
      "0          Income    0.059013\n",
      "3            Race    0.057709\n",
      "11            LDL    0.035392\n",
      "10           Trig    0.033668\n",
      "14       Diabetes    0.019218\n",
      "5       Diastolic    0.016505\n",
      "7           Pulse    0.015389\n",
      "15  CurrentSmoker    0.004574\n",
      "4             Edu    0.004267\n",
      "16       isActive    0.002290\n",
      "17      isInsured    0.001003\n",
      "1             Sex    0.000320\n"
     ]
    }
   ],
   "source": [
    "# feature importance\n",
    "print(\"accuracy for train:\", clf.score(X_train, y_train)*100)\n",
    "\n",
    "# ranked based on the average impurity decrease across all the decision trees in the forest\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display the feature importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': featurenames, 'Importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(\"\\n\", feature_importance_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
