{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10eb7d06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T23:01:45.821078Z",
     "iopub.status.busy": "2024-03-18T23:01:45.820392Z",
     "iopub.status.idle": "2024-03-18T23:01:46.699056Z",
     "shell.execute_reply": "2024-03-18T23:01:46.696791Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gradient boosting is a machine learning technique based on boosting in a functional space, where the target \n",
    "# is pseudo-residuals rather than the typical residuals used in traditional boosting. It gives a prediction \n",
    "# model in the form of an ensemble of weak prediction models, i.e., models that make very few assumptions about \n",
    "# the data, which are typically simple decision trees.[wiki]\n",
    "\n",
    "# Gradient Boosted Decision Trees is a generalization of boosting to arbitrary differentiable loss functions\n",
    "# https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosted-trees\n",
    "\n",
    "# class sklearn.ensemble.GradientBoostingClassifier(*, loss='log_loss', learning_rate=0.1, n_estimators=100, \n",
    "# subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "# max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, \n",
    "# max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd21db51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T23:01:46.707889Z",
     "iopub.status.busy": "2024-03-18T23:01:46.707110Z",
     "iopub.status.idle": "2024-03-18T23:01:52.458813Z",
     "shell.execute_reply": "2024-03-18T23:01:52.456501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for test ......  75.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for test ......  64.70588235294117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for test ......  67.64705882352942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for test ......  67.64705882352942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for test ......  70.58823529411765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for test ......  70.58823529411765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for test ......  61.76470588235294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for test ......  70.58823529411765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for test ......  64.70588235294117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for test ......  76.47058823529412\n",
      "* Average accuracy *:  68.97058823529413\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#input dataset\n",
    "fruit_df = pd.read_csv(\"NHANES_data_stroke_train.csv\")\n",
    "\n",
    "#under sample the non-stroke\n",
    "MI_positive = fruit_df[fruit_df['stroke'] == 1]\n",
    "MI_negitive = fruit_df[fruit_df['stroke'] == 2].sample(frac=.03411675511751327)\n",
    "fruit_df = pd.concat([MI_positive, MI_negitive])\n",
    "\n",
    "fruit_featureNames = [\"Income\",\"Sex\",\"Age\",\"Race\",\"Edu\",\"Diastolic\",\"Systolic\",\"Pulse\",\"BMI\",\"HDL\",\"Trig\",\"LDL\",\"TCHOL\",\"kidneys_eGFR\",\"Diabetes\",\"CurrentSmoker\",\"isActive\",\"isInsured\"]\n",
    "X = fruit_df[fruit_featureNames]\n",
    "y = fruit_df[\"stroke\"]\n",
    "\n",
    "# Define the preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values\n",
    "    ('scaler', StandardScaler())  # Scale the features\n",
    "])\n",
    "\n",
    "# Fit and transform the preprocessing pipeline\n",
    "X = preprocessing_pipeline.fit_transform(X)\n",
    "\n",
    "avgAccuracy = []\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y) \n",
    "    # n_estimators = 100 <--- # of trees\n",
    "    \n",
    "    # Create a Gradient Boosting classifier with regularization\n",
    "    gbt = GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=2,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        subsample=0.8,\n",
    "        verbose=0,\n",
    "    )\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    gbt.fit(X_train, y_train)\n",
    "    \n",
    "    #print(\"accuracy for train:\", clf.score(X_train, y_train)*100)\n",
    "    acc = gbt.score(X_test, y_test)*100\n",
    "    avgAccuracy = avgAccuracy+[acc]\n",
    "    print(\"accuracy for test ...... \", acc)\n",
    "\n",
    "print(\"* Average accuracy *: \", sum(avgAccuracy)/len(avgAccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81f7324",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T23:01:52.467513Z",
     "iopub.status.busy": "2024-03-18T23:01:52.466777Z",
     "iopub.status.idle": "2024-03-18T23:01:53.038220Z",
     "shell.execute_reply": "2024-03-18T23:01:53.035919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for train: 100.0\n",
      "\n",
      "           Feature  Importance\n",
      "13   kidneys_eGFR    0.274765\n",
      "2             Age    0.152181\n",
      "0          Income    0.106107\n",
      "12          TCHOL    0.071210\n",
      "6        Systolic    0.066189\n",
      "11            LDL    0.063173\n",
      "8             BMI    0.047464\n",
      "5       Diastolic    0.044583\n",
      "3            Race    0.042969\n",
      "9             HDL    0.033641\n",
      "10           Trig    0.032668\n",
      "7           Pulse    0.029736\n",
      "14       Diabetes    0.020088\n",
      "4             Edu    0.015151\n",
      "1             Sex    0.000047\n",
      "17      isInsured    0.000027\n",
      "15  CurrentSmoker    0.000000\n",
      "16       isActive    0.000000\n"
     ]
    }
   ],
   "source": [
    "# feature importance\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y) \n",
    "# n_estimators = 100 <--- # of trees\n",
    "\n",
    "gbt = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gbt.fit(X_train, y_train)\n",
    "    \n",
    "print(\"accuracy for train:\", gbt.score(X_train, y_train)*100)\n",
    "\n",
    "# contribution of a feature in each tree is determined by the improvement it brings to the loss function.\n",
    "# importance of a feature is calculated by summing up the impurity reductions \n",
    "feature_importances = gbt.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display the feature importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': fruit_featureNames, 'Importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(\"\\n\", feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054b19d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
