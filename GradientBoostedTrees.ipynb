{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10eb7d06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T23:01:45.821078Z",
     "iopub.status.busy": "2024-03-18T23:01:45.820392Z",
     "iopub.status.idle": "2024-03-18T23:01:46.699056Z",
     "shell.execute_reply": "2024-03-18T23:01:46.696791Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gradient boosting is a machine learning technique based on boosting in a functional space, where the target \n",
    "# is pseudo-residuals rather than the typical residuals used in traditional boosting. It gives a prediction \n",
    "# model in the form of an ensemble of weak prediction models, i.e., models that make very few assumptions about \n",
    "# the data, which are typically simple decision trees.[wiki]\n",
    "\n",
    "# Gradient Boosted Decision Trees is a generalization of boosting to arbitrary differentiable loss functions\n",
    "# https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosted-trees\n",
    "\n",
    "# class sklearn.ensemble.GradientBoostingClassifier(*, loss='log_loss', learning_rate=0.1, n_estimators=100, \n",
    "# subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "# max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, \n",
    "# max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd21db51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T23:01:46.707889Z",
     "iopub.status.busy": "2024-03-18T23:01:46.707110Z",
     "iopub.status.idle": "2024-03-18T23:01:52.458813Z",
     "shell.execute_reply": "2024-03-18T23:01:52.456501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for train: 88.11881188118812\n",
      "accuracy for test ......  73.52941176470588\n",
      "accuracy for train: 83.16831683168317\n",
      "accuracy for test ......  79.41176470588235\n",
      "accuracy for train: 87.12871287128714\n",
      "accuracy for test ......  73.52941176470588\n",
      "accuracy for train: 87.62376237623762\n",
      "accuracy for test ......  69.11764705882352\n",
      "accuracy for train: 85.64356435643565\n",
      "accuracy for test ......  79.41176470588235\n",
      "accuracy for train: 84.65346534653465\n",
      "accuracy for test ......  80.88235294117648\n",
      "accuracy for train: 86.63366336633663\n",
      "accuracy for test ......  64.70588235294117\n",
      "accuracy for train: 84.15841584158416\n",
      "accuracy for test ......  66.17647058823529\n",
      "accuracy for train: 87.12871287128714\n",
      "accuracy for test ......  73.52941176470588\n",
      "accuracy for train: 85.14851485148515\n",
      "accuracy for test ......  70.58823529411765\n",
      "* Average accuracy *:  73.08823529411765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values\n",
    "    ('scaler', StandardScaler())  # Scale the features\n",
    "])\n",
    "\n",
    "# Define the Gradient Boosting model\n",
    "gbt = GradientBoostingClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=2,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    subsample=0.8,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "##############################  TRAINING  ##############################################\n",
    "#input dataset\n",
    "input_df = pd.read_csv(\"NHANES_data_stroke_train.csv\")\n",
    "\n",
    "# Under sample the non-stroke\n",
    "# Due to the large number of MI_positive, drop any with missing values, MI_negative will be imputed later\n",
    "MI_positive = input_df[input_df['stroke'] == 1]\n",
    "MI_negative = input_df[input_df['stroke'] == 2]\n",
    "MI_negative = MI_negative.dropna()\n",
    "MI_negative = MI_negative.sample(n=len(MI_positive), replace=False)\n",
    "input_df = pd.concat([MI_positive, MI_negative])\n",
    "\n",
    "# attributes\n",
    "featurenames = [\"Income\",\"Age\",\"Race\",\"Diastolic\",\"Systolic\",\"Pulse\",\"BMI\",\"HDL\",\"Trig\",\"LDL\",\"TCHOL\",\"kidneys_eGFR\",\"Diabetes\"]\n",
    "X = input_df[featurenames]\n",
    "y = input_df[\"stroke\"]\n",
    "\n",
    "# impute and scale the data\n",
    "X = preprocessing_pipeline.fit_transform(X)\n",
    "\n",
    "avgAccuracy = []\n",
    "for i in range(10):\n",
    "    # split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y) \n",
    "    \n",
    "    # train the model\n",
    "    gbt.fit(X_train, y_train)\n",
    "    \n",
    "    # print accuracy info\n",
    "    print(\"accuracy for train:\", gbt.score(X_train, y_train)*100)\n",
    "    acc = gbt.score(X_test, y_test)*100\n",
    "    avgAccuracy = avgAccuracy+[acc]\n",
    "    print(\"accuracy for test ...... \", acc)\n",
    "\n",
    "print(\"* Average accuracy *: \", sum(avgAccuracy)/len(avgAccuracy))\n",
    "\n",
    "##############################  PREDICTION  ##############################################\n",
    "# load data set\n",
    "new_data = pd.read_csv(\"NHANES_data_stroke_test4Students.csv\")\n",
    "\n",
    "# No stroke column so get rid of it\n",
    "new_data = new_data.drop(columns=['stroke'])\n",
    "\n",
    "# get attributes\n",
    "X_new = new_data[featurenames]\n",
    "\n",
    "# imputer\n",
    "X_new = preprocessing_pipeline.fit_transform(X_new)\n",
    "\n",
    "# Make predictions on the new data, run model\n",
    "new_probabilities = gbt.predict_proba(X_new)[:, 0]  # for output\n",
    "new_predictions = gbt.predict(X_new) # unsed, just for testing ratio of MI/noMI\n",
    "\n",
    "# Get each sample's ID and write probabilities to the output CSV\n",
    "new_participant_ids = new_data['ParticipantID']\n",
    "new_output_df = pd.DataFrame({'ParticipantID': new_participant_ids, 'Pred_Probability': new_probabilities})\n",
    "new_output_df.to_csv('GBTpred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81f7324",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T23:01:52.467513Z",
     "iopub.status.busy": "2024-03-18T23:01:52.466777Z",
     "iopub.status.idle": "2024-03-18T23:01:53.038220Z",
     "shell.execute_reply": "2024-03-18T23:01:53.035919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for train: 100.0\n",
      "\n",
      "           Feature  Importance\n",
      "13   kidneys_eGFR    0.274765\n",
      "2             Age    0.152181\n",
      "0          Income    0.106107\n",
      "12          TCHOL    0.071210\n",
      "6        Systolic    0.066189\n",
      "11            LDL    0.063173\n",
      "8             BMI    0.047464\n",
      "5       Diastolic    0.044583\n",
      "3            Race    0.042969\n",
      "9             HDL    0.033641\n",
      "10           Trig    0.032668\n",
      "7           Pulse    0.029736\n",
      "14       Diabetes    0.020088\n",
      "4             Edu    0.015151\n",
      "1             Sex    0.000047\n",
      "17      isInsured    0.000027\n",
      "15  CurrentSmoker    0.000000\n",
      "16       isActive    0.000000\n"
     ]
    }
   ],
   "source": [
    "# feature importance\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y) \n",
    "# n_estimators = 100 <--- # of trees\n",
    "\n",
    "gbt = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gbt.fit(X_train, y_train)\n",
    "    \n",
    "print(\"accuracy for train:\", gbt.score(X_train, y_train)*100)\n",
    "\n",
    "# contribution of a feature in each tree is determined by the improvement it brings to the loss function.\n",
    "# importance of a feature is calculated by summing up the impurity reductions \n",
    "feature_importances = gbt.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display the feature importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': featurenames, 'Importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(\"\\n\", feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054b19d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
